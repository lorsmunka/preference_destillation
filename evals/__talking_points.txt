text-sentiment-classifier-200m-gemma-3-4b-distill

Cél:
Apply general intelligence to specific tasks, train with distillation to reduce load.

Pipeline:
500k reddit comments (between 3, 25 tokens) -> classify with Gemma 3 4b, get logits -> train with KL + CL, first 100% KL, then reduce KL gradually.

Tapasztalat (~4 days):
- 10 epoch, pure KL, 10 batch, loss 4,5 -> 1.5
- 1 epoch pure CL, 50 batch, loss 1.5 -> 1.3
- 9 epoch, 150, 250, 500 batch + added CE loss -> 1.3 -> 0.7
- accuracy -> ~0.08 -> ~0.20

- learnt basic JSON structure (positionally aware)
- sometimes generates coherent bits

- same input vocab as Gemma 3 4b
- 93 token output vocab (json structure + auxiliary)

- 75 sequence length

- chinchilla scaling 200m params + 270m = 9.4b tokens
- distillation -> 2b tokens -> 1 562 500 (2b / 32 / 40) batches (jelenleg 500 van)  
- 4070:
    - distillation: 875000 sec-> ~10 days 
    - training: 2500000 (5 epoch) sec -> ~29 days
    - total: ~39 days

=== Model Parameter Overview ===
Total trainable parameters: 470,149,213
Embeddings (input + positional): 268,512,256 (268,435,456 + 76,800)
Transformer Attention parameters: 67,174,400
Transformer Feed-Forward parameters: 134,299,648
Transformer Block LayerNorms: 65,536
Final LayerNorm parameters: 2,048
Output projection parameters: 95,325

Sum of all above: 470,149,

class Transformer(nn.Module):
    def __init__(
        self,
        hidden_dim: int = 1024,
        num_layers: int = 16,
        num_heads: int = 16,
        max_seq_length: int = 75,
        dropout: float = 0.1,
        low_rank_dim: int = 256
    ):
    
    - GeLU activations
    - AdamW optimizer
    - Cosine Annealing LR scheduler

Javítások:
    - more auxiliary (CE bevezetésekor 0.8 KL - ~ 0.9 CE)
    - better batch handling, only save a vector for logits
    - better telementry / fix accuracy tracking
    - mixed precision for training
    - use pre trained embeddings
    - 5090? -> valaki átnézi
    - framework to only predict NECESSARY tokens

    - no MLA
    - no mixture of experts
