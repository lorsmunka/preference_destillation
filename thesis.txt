Domain Specifikus LLM Desztillációs Pipeline

Abstract
Sok feladatra szeretnénk az LLM-ek általános intelligenciáját alkalmazni. Gyakran használt LLM alapú ágensrendszereknél (pl.: döntési pontok) és strukturált output generálásnál (pl.: text sentiment classification) hasznos lehet egy desztillációs optimalizálást végrehajtani, hiszen az adott feladatot akár 10-100-szor hatékonyabban fogja tudni elvégezni a desztillált modell. A dolgozat vizsgálja az ilyen rendszerek generalizált megvalósíthatóságát, illetve olyan optimalizációs eszközöket használ, mint a csökkentett kimeneti szótár, csökkentett kontextusméret és csökkentett paraméterszám. A tanítás során egy kisebb transformer modell kerül betanításra curriculum traininget alkalmazva, ahol a modell többfajta loss metrika (Kullback-Leibler divergencia, cross entropy) arányváltozásán keresztül tanul: először a tanármodell eloszlását, majd a következő token pontos eltalálására fektet nagyobb hangsúlyt. A gradiensben végig megjelenik mindkét loss. Az ilyen optimalizálások elérhetővé tehetik nagyobb modellek domain-specifikus tudását akár consumer grade hardveren, vagy jelentősen csökkenthetik az LLM-ek vállalati felhasználási költségeit, miközben a válaszidő is javulhat.

1. Bevezetés
A nagy nyelvi modellek (LLM-ek) az elmúlt években jelentős áttörést hoztak a természetes nyelvfeldolgozásban. Képesek szöveget generálni, klasszifikálni, strukturált outputot előállítani. Ugyanakkor ezek a modellek erőforrásigényesek: drága hardvert, sok memóriát és energiát igényelnek. Sok domain-specifikus feladathoz - ahol a modellnek csak egy szűk területen kell jól teljesítenie - túlzás egy többmilliárd paraméteres általános célú modellt használni.
A dolgozat célja egy olyan desztillációs pipeline kifejlesztése, amely lehetővé teszi nagy nyelvi modellek domain-specifikus tudásának kisebb, hatékonyabb modellekbe történő átültetését. A rendszer úgy lett tervezve, hogy könnyen adaptálható legyen különböző architektúrákra, teacher modellekre és problémakörökre.

1.1 Probléma és motiváció
Az LLM-ek használata egyszerű, jól definiált feladatokra gyakran nem gazdaságos. Egy sentiment klasszifikáció vagy strukturált JSON generálás nem igényli egy több száz milliárd paraméteres modell teljes kapacitását, mégis sok vállalat és fejlesztő ilyen modelleket használ API hívásokon keresztül, magas költségek mellett, hiszen az általános jellegű intelligencia, amellyel ezek a modellek dolgoznak, túlzóan vonzónak tűnhet.
A desztilláció alternatívát kínál: egy kisebb modell megtanulhatja a nagyobb modell viselkedését egy adott feladatra, és azt töredék erőforrással képes végrehajtani. Ez lehetővé teszi:
- on-device futtatást (pl. telefonon, edge eszközökön)
- jelentősen alacsonyabb latenciát és költséget
- függetlenséget külső API-któl
A dolgozatban Reddit komment klasszifikációt használok teszt esetként, de a cél egy olyan rendszer, ami könnyen alkalmazható más problémákra is: lokális matematikai egyenlet megoldás, email szeparálás, on-device tartalommoderáció, vagy bármilyen domain-specifikus feladat, ahol egy nagy modell tudását szeretnénk kisebb formában elérhetővé tenni.
A desztilláció előtt költségelemzés szükséges: elég gyakran hívják-e az adott node-ot ahhoz, hogy megérje a desztillációs befektetés? Egy adott architektúra jobb-e egy másik feladatra? 
A dolgozat nem egy kész framework, hanem egy kutatási kiindulópont, amely segítségével tesztelhetők különböző desztillációs megközelítések: megéri-e egyáltalán, hogyan érdemes csinálni, milyen architektúrák működnek jobban. A kód könnyen módosítható az adott igényekhez (feladat, tanár vagy szülő model cseréje).

2. Implementáció
A projekt 3 részre bontható: data extraction pipeline, tanítás és a modell(ek). A data extraction pipeline egy tanármodellt használva állítja elő és menti el a desztillációs tanításhoz szükséges adatokat (ami akár több modell tanításához is újrahasználható). A tanítás és modell jobban összefügg, a tanítás egy speciális curriculum, amely célja először a teacher modell következő token predikció disztribúciójának a student modellhez való közelítése, majd ezt követően a következő helyes token fontosságára nagyobb hangsúlyt fektetve megerősíteni a modellt, hogy ne csak hasonlóan gondolkodjon, mint a teacher modell, de a helyes döntést is hozza meg. Több fajta student modellt is lehet használni, a dolgozat viszont elsősorban kisebb multi-headed attentiont használó Transformer modellekre fog fókuszálni, amelyek ugyanazzal az autoregresszív viselkedéssel állítják elő a kimeneti választ, mint a manapság leginkább elterjedt nagy nyelvi modellek (pl.: OpenAI GPT, Google Gemini, Anthropic Claude modellek) és a desztillációhoz használt teacher modellek maguk is.

2.1 Példa
A projektet számos feladat optimalizálására lehet használni, ügyfélszolgálati emailek automatikus LLM-ekkel történő osztályozásától, online fórumokon autómoderálásán keresztül, ágensalapú workflow-okig. A dolgozat során Reddit komment sentiment analízisét fogom példának használni, vagyis a feladat az, hogy egy beérkező Reddit kommentet JSON objektummal jellemezze a modell (példa: 1. kép).


1. Kép

2.2 Data extraction pipeline
A projekt során a Google Gemma 3 4b open source modellt használtam mint teacher. A modell méretéhez képest elfogadható intelligenciával rendelkezik, képes megbízhatóan structured outputot (JSON) előállítani. Mérete miatt kényelmesen elfér 12GB VRAM-on, és consumer grade videókártyán jó sebességgel használható inferenciára. A projektben csak a 3 és 25 token hosszúságú, speciális tageket és URL-eket nem tartalmazó kommenteket vizsgáltam. A 3 és 25 token közötti mondatok vizsgálata előre ismertté teszi a maximum context lengthet, így ez is használható az optimalizálás során. A tagek és URL-ek nem figyelembevétele pedig azért szükséges, hogy a modellek ne kapjanak zavaró, félrevezető adatokat.
A mondatokat Gemma 3 4b klasszifikálja, és a klasszifikálás során minden autoregresszív lépésnél mentésre kerülnek a szükséges és auxiliary tokenek nyers logit értékei. A szükséges tokenek azok a tokenek, amelyek abszolút szükségesek az összes lehetséges valid JSON generálásához (~32 token), míg az auxiliary tokenek (whitespace, prompt tokenek, gyakori angol szavak) nagyobb puha címkét biztosítanak a student modellnek, így jobban el tudja sajátítani a teacher "dark knowledge"-jét (~493 auxiliary token, összesen ~525 token). A kimeneti szótár mérete lineárisan skálázódik a compute-tal. Gemma vocab mérete 262,144 token - ehhez képest 32 tokenre csökkentés 99,988%-os redukció, míg 525 tokenre 99,80%-os. A különbség mindössze ~0,19%, de az auxiliary tokenekkel ~16x (1540%) gazdagabb információt kap a student modell - ezért a nagyobb puha címke meghagyható. Tehát egy klasszifikációs példa generálása során 37-42 lépés történik, és minden egyes lépésnél mentésre kerül a teacher modell ~525 figyelt token nyers logit eloszlása. Ezek a példák 128-asával mentésre kerülnek egy JSONL batchbe.
Az, hogy Gemma 3 4b viszonylag gyakran félreklasszifikál mondatokat, nem jelent problémát, hiszen nem azt szeretném bizonyítani, hogy a desztillált modell emberi szempontból helyes választ adott-e, hanem hogy mennyire tudja a student modell követni a teacher modell preferenciáját.
Gemma 3 4b egy felső középkategóriás gépen, egy Nvidia RTX 4070-es videókártyán torch.bfloat16 pontossággal 58-62 másodperc alatt generál 32 példát, példánként átlagosan 40 lépéssel.
2.3 A tanítás
A tanítás egyik legfontosabb része a veszteség, amit használva a modell optimalizál. A projekt két veszteség együttesét használja, változó arányban. A Kullback-Leibler divergencia veszteség két valószínűségi eloszlás közötti különbséget számol, tehát azt mondja meg, hogy a teacher és a student modell mennyire gondolkodnak hasonlóan. A student modell nem csak a helyes választ látja, de a teacher modell eloszlásából a bizonyosságot is, így a sokkal információgazdagabb példából tud a modell tanulni. A tanítás későbbi részében a cross entropy loss kap nagyobb hangsúlyt, amely a helyes következő tokenre optimalizál. A KL divergencia veszteség szerepe elsősorban, hogy gyorsabban tudjon a modell tanulni, kevesebb adatra és tanítási időre legyen szükség. A végső célt azonban, a helyes következő token predikciót a CE loss fogja megerősíteni. A tanítás elején 90%-10% arányban van KL és CE veszteség használva, a tanítás végén az arány megfordul, 10%-90%-ra. Az arány változása cosine annealing-et követ, hasonlóan a learning rate-hez. Mindkét veszteség végig jelen van a gradiensben, így stabilabb a tanulás.
Nagy nyelvi modelleknél szinte kizárólagosan AdamW-t használnak optimizerként, hiszen stabilabb tanulást eredményez (főleg nagyobb modelleknél) és jobban kezeli a sparse adatokat (mint például a nyelv). Ráadásul a modern AI/LLM infrastruktúra többnyire erre az optimizerre van optimalizálva, ez az industry standard.
Hogy a modell kezdetben gyorsabban tudjon tanulni, illetve legyen lehetősége fine-tuneolni a tanítás végére, Cosine Annealinget használtam Learning Rate Schedulernek.
A tanítás folyamán folyamatosan látjuk a konzolon a train losst, a train accuracyt és az adott példa vagy batch idejét. Ezeknek az adatoknak a fontosabb része mentésre is kerül.
Minden epoch végén készül checkpoint és tesztelésre kerül a modell a teljes adathalmaz 10%-án, ami erre a célra lett félretéve.
A program kezeli a graceful shutdownt, tehát epochok között is el tudja menteni a progresst.
2.4 Student modell
Az első, kiinduló modellnél fontos volt, hogy a lehető legnagyobb eséllyel legyen sikeres a desztilláció. A fő kérdések:
- Működik-e a pipeline end-to-end? Elérhető-e közel 100%-os student only accuracy?
- Tud-e koherens JSON-t generálni a saját modell? Egyáltalán a rendelkezésre álló erőforrásokon betanítható-e?
- Működik-e a csökkentett kimeneti szótár?
Ezért a legkedvezőbb körülményeket próbáltam biztosítani: nagy puha címke (~525 token), elegendő paraméter (~530M), Gemma-szerű architektúra. A kimeneti szótár mérete lineárisan skálázódik, így a nagyobb puha címke meghagyható - segít a modellnek magába szívni a teacher "dark knowledge"-ét. A context length négyzetesen skálázódik, így a 75 token ideális balance, de nagyobb sem lenne feltétlenül probléma.
Később más modellek, más architektúrák is tesztelhetők - ez az első modell a proof of concept.
2.4.1 Architektúra és Gemma hasonlóságok
A student modell összesen ~530 millió paraméterből áll:
- Input embedding: ~262M (ugyanaz a tokenizer, mint Gemma 3 4b)
- Attention rétegek: ~67M
- Feed-forward rétegek: ~201M
- Output projection + normalizáció: ~1M
Az input embedding (~262M) viszonylag fix költség: a tokenizer méretétől függ, nem a modell kapacitásától. Jelen felállásban, ahol korlátozott erőforrásokkal kis modellt kis architektúrába desztillálok, az embedding aránytalanul nagynak tűnik (a paraméterek ~50%-a). Nagyobb léptékű desztillációnál (pl. 100B → 1B) az embedding ugyanekkora maradna, de a modell többi része jóval nagyobb lenne, így az arány kedvezőbb. A jelenlegi cél nem az optimális paraméter-hatékonyság, hanem annak bizonyítása, hogy ez a fajta desztillációs training működik: a teacher modell tudása átvihető egy eltérő, kisebb architektúrájú student modellbe.
A modell a következő architekturális elemeket veszi át Gemmától:
- RMSNorm: a hagyományos LayerNorm helyett, ahogy Gemma is használja
- RoPE (Rotary Position Embedding): pozíció kódolásra, Gemma-kompatibilis
- GeGLU-szerű aktiváció: GELU(gate) * linear, hasonló a Gemma megoldásához
- Ugyanaz a tokenizer: a bemeneti tokenek azonos reprezentációt kapnak
Ezek a hasonlóságok biztosítják, hogy a student modell "felfogása" közel áll a teacher modelléhez, így a desztilláció hatékonyabb.
2.4.2 Optimalizációk
A modell bemeneti szótára megegyezik Gemma 3 4b-vel (262,144 token), így minden kommentet, amit Gemma 3 4b ki tud értékelni, a student modell is. A kimeneti szótár viszont jelentősen le van csökkentve: a 262,144 token helyett mindössze ~525 tokenre korlátozódik (32 szükséges + ~493 auxiliary). Ez ~99,8%-os csökkentés.
A context length 75-re van csökkentve (25 token bemenet + 50 token kimenet), szemben Gemma 3 4b 128k context window-jával.
A teacher modell 4 milliárd paraméteres, a student ~530 millió, tehát ~7,5x kisebb modellt kapunk. Ez a méretbeli különbség tükröződik a sebességben is: egy MacBook Pro M1 Pro 32GB rendszeren Gemma 3 4b 1,7 TPS teljesítményt ér el, míg a student modell 12,5 TPS-t. Nvidia RTX 4070-en is érezhető a gyorsulás.
2.4.3 Modell részletek
A modellnek a hidden dimensionja 1024, fele Gemma 3 4b-nek, de továbbra is gazdag reprezentációt biztosít. 16 rétege van, rétegenként 16 attention head-del.
3 Módszertan
3.1 Pilot run
A teljes modell betanítások előtt saját hardveren futtattam pilot runokat, hogy lássam, hogy a kódom jól működik és a modell elkezd-e konvergálni. Egy 30 perces teszt pilot 10 batch adaton már megmutatta, hogy a modell képes helyes JSON outputot generálni. Egy 3 órás pilot run 100 batch adaton, 3,5 epoch után egy 10 batch-es (10*32 példa) teszten a student only accuracy átlagosan 87,69%, míg a teacher forced accuracy átlagosan 98,97% volt. Példa a tesztből, ahol a modell tökéletesen eltalálta a teacher modell klasszifikációját (student only accuracy: 100%):

Bemenet: "F**k you and your family"
Kimenet:
```json
{
  "tone": "aggressive",
  "sentiment": "negative",
  "safety": "harmful",
  "toxicity": "toxic"
}
```

Példa, ahol a modell hibázott (student only accuracy: 69,23%, teacher forced accuracy: 97,44%):

Bemenet: "Bro no way you did this to me again"
Modell kimenet:
```json
{
  "tone": "neutral",
  "sentiment": "neutral",
  "safety": "safe",
  "toxicity": "respectful"
}
```
Teacher kimenet (ground truth):
```json
{
  "tone": "aggressive",
  "sentiment": "negative",
  "safety": "harmful",
  "toxicity": "toxic"
}
```

A második példán látható, hogy a modell hibázott a klasszifikációban, de valid JSON struktúrát generált.
3.2 Korábbi pilot
Egy korábbi, nagyobb léptékű pilot run során 97 órán keresztül tanítottam a modellt. A loss folyamatosan csökkent, a teacher forced accuracy 80-90% körül volt, viszont a student only accuracy csak 10-20% maradt. A nagy különbség a két metrika között gyanús volt. A generált JSON outputok hibásak voltak, annak ellenére, hogy a loss és a teacher forced accuracy jónak tűnt. Később kiderült, hogy egy encoding hiba okozta a problémát: néhány token (pl. „_\"") rosszul volt kódolva a training adatban és inference közben is. A hiba javítása után a 3.1-ben leírt eredményeket kaptam.
3.3 Korai becslés
Saját hardveren (felső középkategóriás gép, RTX 4070 12GB videokártya) a teljes training és adatelőállítási idő 2870 óra lenne.
Az adatbázisban ~500 ezer Reddit komment van, ami megfelel minden kitételnek. Ezekből a példamondatokból saját hardveren (~62 másodperc / 32 példa) optimális esetben 270 óra alatt lehetne egy teljes tanító adatbázist létrehozni.
A "Chinchilla scaling" szerint minden egyes paraméterre 20 tokennyi példa kell a sikeres tanításhoz. Ez ~530 millió modell paraméter esetében ~10 milliárd tokent jelent, viszont az 500 ezer komment legjobb esetben is csak 20 millió példa. A desztillációból fakadó előnyök és a specifikus domain csökkentett tudásigénye mellett is több mint valószínű, hogy az összes példát fog kelleni használni.
15 epoch 500 ezer példán, példánkénti 1,25 sebességgel ideális esetben is 2600 órányi folyamatos GPU idő.
Tehát a pilot teszt alapján egy modell end-to-end tanítás 2870 óra, vagyis 120 nap, innentől kezdve pedig további modellenként 2600 órával, vagyis 108 nappal kell számolni.

