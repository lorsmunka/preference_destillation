Domain Specifikus LLM Desztillációs Pipeline

Abstract
Sok feladatra szeretnénk az LLM-ek általános intelligenciáját alkalmazni. Gyakran használt LLM alapú ágensrendszereknél (pl.: döntési pontok) és strukturált output generálásnál (pl.: text sentiment classification) hasznos lehet egy desztillációs optimalizálást végrehajtani, hiszen az adott feladatot akár 10-100-szor hatékonyabban fogja tudni elvégezni a desztillált modell. A dolgozat vizsgálja az ilyen rendszerek generalizált megvalósíthatóságát, illetve olyan optimalizációs eszközöket használ, mint a csökkentett kimeneti szótár, csökkentett kontextusméret és csökkentett paraméterszám. A tanítás során egy kisebb transformer modell kerül betanításra curriculum traininget alkalmazva, ahol a modell többfajta loss metrika (Kullback-Leibler divergencia, cross entropy) arányváltozásán keresztül tanul: először a tanármodell eloszlását, majd a következő token pontos eltalálására fektet nagyobb hangsúlyt. A gradiensben végig megjelenik mindkét loss. Az ilyen optimalizálások elérhetővé tehetik nagyobb modellek domain-specifikus tudását akár consumer grade hardveren, vagy jelentősen csökkenthetik az LLM-ek vállalati felhasználási költségeit, miközben a válaszidő is javulhat.

1. Bevezetés
TODO

2. Implementáció
A projekt 3 részre bontható: data extraction pipeline, tanítás és a modell(ek). A data extraction pipeline egy tanármodellt használva állítja elő és menti el a desztillációs tanításhoz szükséges adatokat (ami akár több modell tanításához is újrahasználható). A tanítás és modell jobban összefügg, a tanítás egy speciális curriculum, amely célja először a teacher modell következő token predikció disztribúciójának a student modellhez való közelítése, majd ezt követően a következő helyes token fontosságára nagyobb hangsúlyt fektetve megerősíteni a modellt, hogy ne csak hasonlóan gondolkodjon, mint a teacher modell, de a helyes döntést is hozza meg. Több fajta student modellt is lehet használni, a dolgozat viszont elsősorban kisebb multi-headed attentiont használó Transformer modellekre fog fókuszálni, amelyek ugyanazzal az autoregresszív viselkedéssel állítják elő a kimeneti választ, mint a manapság leginkább elterjedt nagy nyelvi modellek (pl.: OpenAI GPT, Google Gemini, Anthropic Claude modellek) és a desztillációhoz használt teacher modellek maguk is.

2.1 Példa
A projektet számos feladat optimalizálására lehet használni, ügyfélszolgálati emailek automatikus LLM-ekkel történő osztályozásától, online fórumokon autómoderálásán keresztül, ágensalapú workflow-okig. A dolgozat során Reddit komment sentiment analízisét fogom példának használni, vagyis a feladat az, hogy egy beérkező Reddit kommentet JSON objektummal jellemezze a modell (példa: 1. kép).


1. Kép

2.2 Data extraction pipeline
A projekt során a Google Gemma 3 4b open source modellt használtam mint teacher. A modell méretéhez képest elfogadható intelligenciával rendelkezik, képes megbízhatóan structured outputot (JSON) előállítani. Mérete miatt kényelmesen elfér 12GB VRAM-on, és consumer grade videókártyán jó sebességgel használható inferenciára. A projektben csak a 3 és 25 token hosszúságú, speciális tageket és URL-eket nem tartalmazó kommenteket vizsgáltam. A 3 és 25 token közötti mondatok vizsgálata előre ismertté teszi a maximum context lengthet, így ez is használható az optimalizálás során. A tagek és URL-ek nem figyelembevétele pedig azért szükséges, hogy a modellek ne kapjanak zavaró, félrevezető adatokat.
A mondatokat Gemma 3 4b klasszifikálja, és a klasszifikálás során minden autoregresszív lépésnél mentésre kerülnek a szükséges és auxiliary tokenek nyers logit értékei. A szükséges tokenek azok a tokenek, amelyek abszolút szükségesek az összes lehetséges valid JSON generálásához (32 token), míg az auxiliary tokenek csupán azt a célt szolgálják, hogy a desztilláció során nagyobb puha címkét biztosítsanak a student modellnek, így jobban el tudja a student sajátítani a teacher gondolkodását (278 token). Tehát egy klasszifikációs példa generálása során 37-42 lépés történik, és minden egyes lépésnél mentésre kerül a teacher modell 300 figyelt token nyers logit eloszlása. Ezek a példák 128-asával mentésre kerülnek egy JSONL batchbe.
Az, hogy Gemma 3 4b viszonylag gyakran félreklasszifikál mondatokat, nem jelent problémát, hiszen nem azt szeretném bizonyítani, hogy a desztillált modell emberi szempontból helyes választ adott-e, hanem hogy mennyire tudja a student modell követni a teacher modell preferenciáját.
Gemma 3 4b egy felső középkategóriás gépen, egy Nvidia RTX 4070-es videókártyán torch.bfloat16 pontossággal 58-62 másodperc alatt generál 32 példát, példánként átlagosan 40 lépéssel.
2.3 A tanítás
A tanítás egyik legfontosabb része a veszteség, amit használva a modell optimalizál. A projekt két veszteség együttesét használja, változó arányban. A Kullback-Leibler divergencia veszteség két valószínűségi eloszlás közötti különbséget számol, tehát azt mondja meg, hogy a teacher és a student modell mennyire gondolkodnak hasonlóan. A student modell nem csak a helyes választ látja, de a teacher modell eloszlásából a bizonyosságot is, így a sokkal információgazdagabb példából tud a modell tanulni. A tanítás későbbi részében a cross entropy loss kap nagyobb hangsúlyt, amely a helyes következő tokenre optimalizál. A KL divergencia veszteség szerepe elsősorban, hogy gyorsabban tudjon a modell tanulni, kevesebb adatra és tanítási időre legyen szükség. A végső célt azonban, a helyes következő token predikciót a CE loss fogja megerősíteni. A tanítás elején 80%-20% arányban van KL és CE veszteség használva, a tanítás végén az arány megfordul, 20%-80%-ra. Az arány változása lineárisan változik. Mindkét veszteség végig jelen van a gradiensben, így stabilabb a tanulás.
Nagy nyelvi modelleknél szinte kizárólagosan AdamW-t használnak optimizerként, hiszen stabilabb tanulást eredményez (főleg nagyobb modelleknél) és jobban kezeli a sparse adatokat (mint például a nyelv). Ráadásul a modern AI/LLM infrastruktúra többnyire erre az optimizerre van optimalizálva, ez az industry standard.
Hogy a modell kezdetben gyorsabban tudjon tanulni, illetve legyen lehetősége fine-tuneolni a tanítás végére, Cosine Annealinget használtam Learning Rate Schedulernek.
A tanítás folyamán folyamatosan látjuk a konzolon a train losst, a train accuracyt és az adott példa vagy batch idejét. Ezeknek az adatoknak a fontosabb része mentésre is kerül.
Minden epoch végén készül checkpoint és tesztelésre kerül a modell a teljes adathalmaz 10%-án, ami erre a célra lett félretéve.
A program kezeli a graceful shutdownt, tehát epochok között is el tudja menteni a progresst.
2.4.1 Alap modell
A student modell egy 200m paraméteres Transformer architektúrájú modellből és 270m embeddingből áll. A modell bemeneti szótára megegyezik Gemma 3 4b-vel (nagyjából 255k token) és ugyanazt a tokenizert használja. Így jelen példában minden kommentet, amit Gemma 3 4b ki tud értékelni, a saját modell is. A kimeneti szótára viszont jelentősen le van csökkentve Gemma 3 4b-hez képest, a nagyjából 255k token helyett mindössze 32, input generáláshoz szükséges és 278 auxiliary, tanítást segítő, tehát összesen 300 tokenre korlátozódik. Ez körülbelül ~98,5%-os optimalizálás. A saját modellnek a context length-je 75-re van csökkentve, hiszen 25 lehet a maximum hossza a vizsgált Reddit kommentnek és maximum 50 hosszú lehet a generált JSON output. Gemma 3 4b-nek 128k a context windowja, tehát 99,5%-os az optimalizálás.
A teacher modell 4000 millió, vagyis 4 milliárd paraméterszámú, a saját modell 200 millió, tehát egy 20x kisebb modellt kapunk eredményül, ami a kisebb context length miatt több paramétert jelent beolvasott tokenekre. Illetve a modellnek csak egy részét kell tudni annak, amit a teachernek. Ez a különbség tükröződik is TPS-ben (Tokens per second). Egy MacBook Pro M1 Pro 32GB rendszeren Gemma 3 4b 1,7 TPS teljesítményt ér el, míg a saját modell 12,5 TPS-t tudott elérni. Ráadásul ez a méretbeli különbség határozhatja meg, hogy például egy telefonon, vagy régebbi generációs hardveren lehet-e a modellt ésszerű keretek között futtatni.
A modellnek a hidden dimensionja 1024, fele Gemma 3 4b-nek, de továbbra is gazdag reprezentációt biztosítva.
A modellnek 16 rétege van és rétegenként 16 attention headje. A modell paramétereinek nagy része a feed forward MLP rétegekben van (130m), míg a figyelem fejeknek 70m paraméter van allokálva.
Aktivációs függvényként GeLU-t használtam, ez a népszerű ReLU függvények egy folyamatos verziója, gyakran használt LLM-eknél és transformereknél.
3 Módszertan
3.1 Pilot run
A teljes modell betanítások előtt saját hardveren futtattam pilot runokat, hogy lássam, hogy a kódom jól működik és a modell elkezd-e konvergálni. Egy 30 perces teszt pilot 10 batch adaton már megmutatta, hogy a modell képes helyes JSON outputot generálni. Egy 3 órás pilot run 100 batch adaton, 3,5 epoch után egy 10 batch-es (10*32 példa) teszten a student only accuracy átlagosan 87,69%, míg a teacher forced accuracy átlagosan 98,97% volt. Példa a tesztből, ahol a modell tökéletesen eltalálta a teacher modell klasszifikációját (student only accuracy: 100%):

Bemenet: "F**k you and your family"
Kimenet:
```json
{
  "tone": "aggressive",
  "sentiment": "negative",
  "safety": "harmful",
  "toxicity": "toxic"
}
```

Példa, ahol a modell hibázott (student only accuracy: 69,23%, teacher forced accuracy: 97,44%):

Bemenet: "Bro no way you did this to me again"
Modell kimenet:
```json
{
  "tone": "neutral",
  "sentiment": "neutral",
  "safety": "safe",
  "toxicity": "respectful"
}
```
Teacher kimenet (ground truth):
```json
{
  "tone": "aggressive",
  "sentiment": "negative",
  "safety": "harmful",
  "toxicity": "toxic"
}
```

A második példán látható, hogy a modell hibázott a klasszifikációban, de valid JSON struktúrát generált.
3.2 Korábbi pilot
Egy korábbi, nagyobb léptékű pilot run során 97 órán keresztül tanítottam a modellt. A loss folyamatosan csökkent, a teacher forced accuracy 80-90% körül volt, viszont a student only accuracy csak 10-20% maradt. A nagy különbség a két metrika között gyanús volt. A generált JSON outputok hibásak voltak, annak ellenére, hogy a loss és a teacher forced accuracy jónak tűnt. Később kiderült, hogy egy encoding hiba okozta a problémát: néhány token (pl. „_\"") rosszul volt kódolva a training adatban és inference közben is. A hiba javítása után a 3.1-ben leírt eredményeket kaptam.
3.3 Korai becslés
Saját hardveren (felső középkategóriás gép, RTX 4070 12GB videokártya) a teljes training és adatelőállítási idő 2870 óra lenne.
Az adatbázisban ~500 ezer Reddit komment van, ami megfelel minden kitételnek. Ezekből a példamondatokból saját hardveren (~62 másodperc / 32 példa) optimális esetben 270 óra alatt lehetne egy teljes tanító adatbázist létrehozni.
A "Chinchilla scaling" szerint minden egyes paraméterre 20 tokennyi példa kell a sikeres tanításhoz. Ez 200 millió modell paraméter esetében 4 milliárd tokent jelent, viszont az 500 ezer komment legjobb esetben is csak 20 millió példa. A desztillációból fakadó előnyök és a specifikus domain csökkentett tudásigénye mellett is több mint valószínű, hogy az összes példát fog kelleni használni.
15 epoch 500 ezer példán, példánkénti 1,25 sebességgel ideális esetben is 2600 órányi folyamatos GPU idő.
Tehát a pilot teszt alapján egy modell end-to-end tanítás 2870 óra, vagyis 120 nap, innentől kezdve pedig további modellenként 2600 órával, vagyis 108 nappal kell számolni.

